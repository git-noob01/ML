{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 金融风控项目\n",
    "在此项目中，你需要完成金融风控模型的搭建。 基于一个用户的基本信息、历史信息来预测逾期与否。采样的具体数据是拍拍贷数据。https://www.kesci.com/home/competition/56cd5f02b89b5bd026cb39c9/content/1\n",
    "在此数据中提供了三种不同类型的数据:\n",
    "1. Master: 用户的主要信息\n",
    "2. Loginfo: 登录信息\n",
    "3. Userupdateinfo: 修改信息\n",
    "\n",
    "在本次项目中，我们只使用```Master```的信息来预测一个用户是否会逾期。 数据里有一个字段叫作```Target```是样本的标签（label)。 在```Master```表格里，包含200多个特征，但不少特征具有缺失值。 做项目的时候需要仔细处理一下。 \n",
    "\n",
    "对于特征处理方面的技术，请参考本章视频课程里的内容。\n",
    "\n",
    "本项目区别于之前的项目，希望大家能够开放性地思考，不要太局限于给定的条条框框，把目前为止学到的知识都用起来。所以在项目的设计上区别于之前的，没有设置太多的框架性的，大家可以按照自己的思路灵活做项目。 由于项目本身来自于数据竞赛，所以可以试着跟竞赛里的TOP选手的结果做一下对比，看看跟他们的差距或者优势在哪儿。\n",
    "\n",
    "```数据```\n",
    "- ```Training/PPD_Training_Master_GBK_3_1_Training_Set.csv```:  训练数据\n",
    "- ```Test/PPD_Master_GBK_2_Test_Set.csv```: 测试数据\n",
    "\n",
    "\n",
    "强调：一定要把注释写清楚。 每个函数，每一个模块具体做什么写清楚。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 注意： 除了下面导入的库，还有sklearn、XGBoost等经典的库之外，建议不要使用其他的函数库。 如果你不得不使用一些其他特殊的库，请把环境注明在requirements.txt里面，不然我们判作业的时候就没有办法去评判了。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import math \n",
    "import pandas as pd \n",
    "pd.set_option('display.float_format',lambda x:'%.3f' % x)\n",
    "import matplotlib.pyplot as plt \n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "import seaborn as sns \n",
    "sns.set_palette('muted')\n",
    "sns.set_style('darkgrid')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 228)\n"
     ]
    }
   ],
   "source": [
    "# 读取Master数据\n",
    "data = pd.read_csv('data/Training/PPD_Training_Master_GBK_3_1_Training_Set.csv',encoding='gb18030')\n",
    "print (data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Idx</th>\n",
       "      <th>UserInfo_1</th>\n",
       "      <th>UserInfo_2</th>\n",
       "      <th>UserInfo_3</th>\n",
       "      <th>UserInfo_4</th>\n",
       "      <th>WeblogInfo_1</th>\n",
       "      <th>WeblogInfo_2</th>\n",
       "      <th>WeblogInfo_3</th>\n",
       "      <th>WeblogInfo_4</th>\n",
       "      <th>WeblogInfo_5</th>\n",
       "      <th>...</th>\n",
       "      <th>SocialNetwork_10</th>\n",
       "      <th>SocialNetwork_11</th>\n",
       "      <th>SocialNetwork_12</th>\n",
       "      <th>SocialNetwork_13</th>\n",
       "      <th>SocialNetwork_14</th>\n",
       "      <th>SocialNetwork_15</th>\n",
       "      <th>SocialNetwork_16</th>\n",
       "      <th>SocialNetwork_17</th>\n",
       "      <th>target</th>\n",
       "      <th>ListingInfo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10001</td>\n",
       "      <td>1.000</td>\n",
       "      <td>深圳</td>\n",
       "      <td>4.000</td>\n",
       "      <td>深圳</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.000</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>222</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2014/3/5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10002</td>\n",
       "      <td>1.000</td>\n",
       "      <td>温州</td>\n",
       "      <td>4.000</td>\n",
       "      <td>温州</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.000</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2014/2/26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10003</td>\n",
       "      <td>1.000</td>\n",
       "      <td>宜昌</td>\n",
       "      <td>3.000</td>\n",
       "      <td>宜昌</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.000</td>\n",
       "      <td>nan</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2014/2/28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10006</td>\n",
       "      <td>4.000</td>\n",
       "      <td>南平</td>\n",
       "      <td>1.000</td>\n",
       "      <td>南平</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2014/2/25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10007</td>\n",
       "      <td>5.000</td>\n",
       "      <td>辽阳</td>\n",
       "      <td>1.000</td>\n",
       "      <td>辽阳</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.000</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2014/2/27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 228 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Idx  UserInfo_1 UserInfo_2  UserInfo_3 UserInfo_4  WeblogInfo_1  \\\n",
       "0  10001       1.000         深圳       4.000         深圳           nan   \n",
       "1  10002       1.000         温州       4.000         温州           nan   \n",
       "2  10003       1.000         宜昌       3.000         宜昌           nan   \n",
       "3  10006       4.000         南平       1.000         南平           nan   \n",
       "4  10007       5.000         辽阳       1.000         辽阳           nan   \n",
       "\n",
       "   WeblogInfo_2  WeblogInfo_3  WeblogInfo_4  WeblogInfo_5  ...  \\\n",
       "0         1.000           nan         1.000         1.000  ...   \n",
       "1         0.000           nan         1.000         1.000  ...   \n",
       "2         0.000           nan         2.000         2.000  ...   \n",
       "3           nan           nan           nan           nan  ...   \n",
       "4         0.000           nan         1.000         1.000  ...   \n",
       "\n",
       "   SocialNetwork_10  SocialNetwork_11  SocialNetwork_12  SocialNetwork_13  \\\n",
       "0               222                -1                 0                 0   \n",
       "1                 1                -1                 0                 0   \n",
       "2                -1                -1                -1                 1   \n",
       "3                -1                -1                -1                 0   \n",
       "4                -1                -1                -1                 0   \n",
       "\n",
       "   SocialNetwork_14  SocialNetwork_15  SocialNetwork_16  SocialNetwork_17  \\\n",
       "0                 0                 0                 0                 1   \n",
       "1                 0                 0                 0                 2   \n",
       "2                 0                 0                 0                 0   \n",
       "3                 0                 0                 0                 0   \n",
       "4                 0                 0                 0                 0   \n",
       "\n",
       "   target  ListingInfo  \n",
       "0       0     2014/3/5  \n",
       "1       0    2014/2/26  \n",
       "2       0    2014/2/28  \n",
       "3       0    2014/2/25  \n",
       "4       0    2014/2/27  \n",
       "\n",
       "[5 rows x 228 columns]"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 展示记录\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    27802\n",
       "1     2198\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 正负样本的比例， 可以看出样本比例不平衡的\n",
    "data.target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好了，剩下的部分需要由大家完成。 我大致给一下思路，然后大家可以按照这个思路去一步步完成。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### 1. 数据的预处。 需要考虑以下几个方面：\n",
    "- ```缺失值```。数据里有大量的缺失值，需要做一些处理。 \n",
    "- ```字符串的清洗```。比如“北京市”和“北京”合并成“北京”， 统一转换成小写等\n",
    "- ```二值化```。具体方法请参考课程里的介绍\n",
    "- ```衍生特征```：比如户籍地和当前城市是否是同一个？ \n",
    "- ```特征的独热编码```：对于类别型特征使用独热编码形式\n",
    "- ```连续性特征的处理```：根据情况来处理\n",
    "- ```其他```: 根据情况，自行决定要不要做"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "字段名为： Idx        缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： UserInfo_1 缺失值数量: 6    缺失数量占比： 0.02%\n",
      "字段名为： UserInfo_2 缺失值数量: 302  缺失数量占比： 1.01%\n",
      "字段名为： UserInfo_3 缺失值数量: 7    缺失数量占比： 0.02%\n",
      "字段名为： UserInfo_4 缺失值数量: 268  缺失数量占比： 0.89%\n",
      "字段名为： WeblogInfo_1 缺失值数量: 29030 缺失数量占比： 96.77%\n",
      "字段名为： WeblogInfo_2 缺失值数量: 1658 缺失数量占比： 5.53%\n",
      "字段名为： WeblogInfo_3 缺失值数量: 29030 缺失数量占比： 96.77%\n",
      "字段名为： WeblogInfo_4 缺失值数量: 1651 缺失数量占比： 5.50%\n",
      "字段名为： WeblogInfo_5 缺失值数量: 1651 缺失数量占比： 5.50%\n",
      "字段名为： WeblogInfo_6 缺失值数量: 1651 缺失数量占比： 5.50%\n",
      "字段名为： WeblogInfo_7 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： WeblogInfo_8 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： WeblogInfo_9 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： WeblogInfo_10 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： WeblogInfo_11 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： WeblogInfo_12 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： WeblogInfo_13 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： WeblogInfo_14 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： WeblogInfo_15 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： WeblogInfo_16 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： WeblogInfo_17 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： WeblogInfo_18 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： UserInfo_5 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： UserInfo_6 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： UserInfo_7 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： UserInfo_8 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： UserInfo_9 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： UserInfo_10 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： UserInfo_11 缺失值数量: 18909 缺失数量占比： 63.03%\n",
      "字段名为： UserInfo_12 缺失值数量: 18909 缺失数量占比： 63.03%\n",
      "字段名为： UserInfo_13 缺失值数量: 18909 缺失数量占比： 63.03%\n",
      "字段名为： UserInfo_14 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： UserInfo_15 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： UserInfo_16 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： UserInfo_17 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： UserInfo_18 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： UserInfo_19 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： UserInfo_20 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： UserInfo_21 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： UserInfo_22 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： UserInfo_23 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： UserInfo_24 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： Education_Info1 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： Education_Info2 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： Education_Info3 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： Education_Info4 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： Education_Info5 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： Education_Info6 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： Education_Info7 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： Education_Info8 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： WeblogInfo_19 缺失值数量: 2963 缺失数量占比： 9.88%\n",
      "字段名为： WeblogInfo_20 缺失值数量: 8050 缺失数量占比： 26.83%\n",
      "字段名为： WeblogInfo_21 缺失值数量: 3074 缺失数量占比： 10.25%\n",
      "字段名为： WeblogInfo_23 缺失值数量: 253  缺失数量占比： 0.84%\n",
      "字段名为： WeblogInfo_24 缺失值数量: 253  缺失数量占比： 0.84%\n",
      "字段名为： WeblogInfo_25 缺失值数量: 253  缺失数量占比： 0.84%\n",
      "字段名为： WeblogInfo_26 缺失值数量: 253  缺失数量占比： 0.84%\n",
      "字段名为： WeblogInfo_27 缺失值数量: 253  缺失数量占比： 0.84%\n",
      "字段名为： WeblogInfo_28 缺失值数量: 253  缺失数量占比： 0.84%\n",
      "字段名为： WeblogInfo_29 缺失值数量: 253  缺失数量占比： 0.84%\n",
      "字段名为： WeblogInfo_30 缺失值数量: 253  缺失数量占比： 0.84%\n",
      "字段名为： WeblogInfo_31 缺失值数量: 253  缺失数量占比： 0.84%\n",
      "字段名为： WeblogInfo_32 缺失值数量: 253  缺失数量占比： 0.84%\n",
      "字段名为： WeblogInfo_33 缺失值数量: 253  缺失数量占比： 0.84%\n",
      "字段名为： WeblogInfo_34 缺失值数量: 253  缺失数量占比： 0.84%\n",
      "字段名为： WeblogInfo_35 缺失值数量: 253  缺失数量占比： 0.84%\n",
      "字段名为： WeblogInfo_36 缺失值数量: 253  缺失数量占比： 0.84%\n",
      "字段名为： WeblogInfo_37 缺失值数量: 253  缺失数量占比： 0.84%\n",
      "字段名为： WeblogInfo_38 缺失值数量: 253  缺失数量占比： 0.84%\n",
      "字段名为： WeblogInfo_39 缺失值数量: 253  缺失数量占比： 0.84%\n",
      "字段名为： WeblogInfo_40 缺失值数量: 253  缺失数量占比： 0.84%\n",
      "字段名为： WeblogInfo_41 缺失值数量: 253  缺失数量占比： 0.84%\n",
      "字段名为： WeblogInfo_42 缺失值数量: 253  缺失数量占比： 0.84%\n",
      "字段名为： WeblogInfo_43 缺失值数量: 253  缺失数量占比： 0.84%\n",
      "字段名为： WeblogInfo_44 缺失值数量: 253  缺失数量占比： 0.84%\n",
      "字段名为： WeblogInfo_45 缺失值数量: 253  缺失数量占比： 0.84%\n",
      "字段名为： WeblogInfo_46 缺失值数量: 253  缺失数量占比： 0.84%\n",
      "字段名为： WeblogInfo_47 缺失值数量: 253  缺失数量占比： 0.84%\n",
      "字段名为： WeblogInfo_48 缺失值数量: 253  缺失数量占比： 0.84%\n",
      "字段名为： WeblogInfo_49 缺失值数量: 253  缺失数量占比： 0.84%\n",
      "字段名为： WeblogInfo_50 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： WeblogInfo_51 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： WeblogInfo_52 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： WeblogInfo_53 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： WeblogInfo_54 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： WeblogInfo_55 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： WeblogInfo_56 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： WeblogInfo_57 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： WeblogInfo_58 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period1_1 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period1_2 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period1_3 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period1_4 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period1_5 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period1_6 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period1_7 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period1_8 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period1_9 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period1_10 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period1_11 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period1_12 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period1_13 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period1_14 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period1_15 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period1_16 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period1_17 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period2_1 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period2_2 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period2_3 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period2_4 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period2_5 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period2_6 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period2_7 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period2_8 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period2_9 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period2_10 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period2_11 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period2_12 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period2_13 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period2_14 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period2_15 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period2_16 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period2_17 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period3_1 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period3_2 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period3_3 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period3_4 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period3_5 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period3_6 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period3_7 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period3_8 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period3_9 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period3_10 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period3_11 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period3_12 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period3_13 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period3_14 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period3_15 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period3_16 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period3_17 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period4_1 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period4_2 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period4_3 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period4_4 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period4_5 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period4_6 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period4_7 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period4_8 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period4_9 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period4_10 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period4_11 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period4_12 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period4_13 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period4_14 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period4_15 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period4_16 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period4_17 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period5_1 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period5_2 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period5_3 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period5_4 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period5_5 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period5_6 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period5_7 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period5_8 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period5_9 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period5_10 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period5_11 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period5_12 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period5_13 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period5_14 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period5_15 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period5_16 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period5_17 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period6_1 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period6_2 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period6_3 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period6_4 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period6_5 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period6_6 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period6_7 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period6_8 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period6_9 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period6_10 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period6_11 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period6_12 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period6_13 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period6_14 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period6_15 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period6_16 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period6_17 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period7_1 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period7_2 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period7_3 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period7_4 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period7_5 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period7_6 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period7_7 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period7_8 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period7_9 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period7_10 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period7_11 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period7_12 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period7_13 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period7_14 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period7_15 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period7_16 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ThirdParty_Info_Period7_17 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： SocialNetwork_1 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： SocialNetwork_2 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： SocialNetwork_3 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： SocialNetwork_4 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： SocialNetwork_5 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： SocialNetwork_6 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： SocialNetwork_7 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： SocialNetwork_8 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： SocialNetwork_9 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： SocialNetwork_10 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： SocialNetwork_11 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： SocialNetwork_12 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： SocialNetwork_13 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： SocialNetwork_14 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： SocialNetwork_15 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： SocialNetwork_16 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： SocialNetwork_17 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： target     缺失值数量: 0    缺失数量占比： 0.00%\n",
      "字段名为： ListingInfo 缺失值数量: 0    缺失数量占比： 0.00%\n",
      "删除了5列的数据\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20988, 223)\n",
      "0    19356\n",
      "1     1632\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 1. 处理缺失值\n",
    "temp = data.isnull().any()\n",
    "\n",
    "# 1）统计每个维度缺失值大小删除缺失值超过30%的特征.\n",
    "dropList1 = []   # 保存需要删除的column名字\n",
    "for colname in data.columns:\n",
    "    nCount = len(data) - data[colname].count()\n",
    "    nLostRate = (nCount / len(data)) * 100\n",
    "    sLostRate = '%.2f%%' % nLostRate\n",
    "    print('字段名为：',str(colname).ljust(10),'缺失值数量:',str(nCount).ljust(4),'缺失数量占比：',sLostRate)\n",
    "    if (nLostRate > 30):\n",
    "        dropList1.append(colname)\n",
    "\n",
    "# 2）删除缺失值大于30%的列\n",
    "data.drop(dropList1, axis=1, inplace=True)\n",
    "print(\"删除了%d列的数据\"%len(dropList1))\n",
    "\n",
    "# 3）删除存在缺失值的行\n",
    "data.dropna(axis=0, how='any', inplace=True)\n",
    "data_test.dropna(axis=0, how='any', inplace=True)\n",
    "print(data.shape)\n",
    "print(data.target.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['深圳' '温州' '宜昌' '吴忠' '绵阳' '东莞' '赤峰' '武汉' '长沙' '漳州' '牡丹江' '北京' '成都' '三明'\n",
      " '临沂' '福州' '泰州' '上海' '红河哈尼族彝族自治州' '南平' '郴州' '常州' '湖州' '茂名' '天津' '南宁' '聊城'\n",
      " '柳州' '太原' '重庆' '曲靖' '合肥' '鸡西' '资阳' '兰州' '济宁' '丽水' '滨州' '渭南' '汕头' '黔南'\n",
      " '廊坊' '西宁' '金华' '龙岩' '清远' '徐州' '潍坊' '阳泉' '包头' '陇南' '保定' '吉安' '厦门' '大庆'\n",
      " '荆门' '威海' '石家庄' '汕尾' '淄博' '巴彦淖尔盟' '黔西南' '昆明' '宝鸡' '酒泉' '延边朝鲜族自治州' '泉州'\n",
      " '无锡' '黄冈' '商丘' '抚州' '吕梁' '阿拉善盟' '黑河' '宿州' '克拉玛依' '淮南' '邵阳' '惠州' '益阳' '淮安'\n",
      " '咸宁' '洛阳' '襄阳' '平顶山' '泰安' '扬州' '新乡' '海口' '西安' '焦作' '唐山' '梅州' '肇庆' '阜阳'\n",
      " '岳阳' '鞍山' '永州' '杭州' '哈尔滨' '郑州' '南阳' '赣州' '绍兴' '济南' '绥化' '蚌埠' '河源' '银川'\n",
      " '南京' '连云港' '韶关' '九江' '广州' '白银' '镇江' '榆林' '广元' '菏泽' '阳江' '日照' '台州' '鄂尔多斯'\n",
      " '沈阳' '常德' '烟台' '中山' '白山' '苏州' '周口' '宁德' '大同' '贵阳' '固原' '德阳' '来宾' '宜宾'\n",
      " '随州' '运城' '衢州' '襄樊' '荆州' '邯郸' '营口' '邢台' '丹东' '玉林' '南充' '莆田' '嘉兴' '乌鲁木齐'\n",
      " '伊犁哈萨克自治州' '玉溪' '晋中' '乌海' '佛山' '定西' '德宏傣族景颇族自治州' '遵义' '北海' '东营' '百色' '巢湖'\n",
      " '怀化' '咸阳' '揭阳' '长治' '三门峡' '乌兰察布盟' '临汾' '昭通' '德州' '平凉' '青岛' '锦州' '朝阳' '枣庄'\n",
      " '安阳' '潮州' '阿克苏' '秦皇岛' '安康' '钦州' '盐城' '佳木斯' '巴中' '漯河' '汉中' '达州' '宿迁' '晋城'\n",
      " '湛江' '濮阳' '珠海' '南昌' '长春' '南通' '信阳' '四平' '萍乡' '孝感' '宜春' '张家口' '鄂州' '黔东南'\n",
      " '呼和浩特' '桂林' '海东' '娄底' '喀什' '眉山' '天水' '泸州' '乐山' '新余' '滁州' '十堰' '马鞍山' '衡阳'\n",
      " '阜新' '河池' '张家界' '丽江' '六安' '江门' '梧州' '景德镇' '攀枝花' '抚顺' '黄山' '宁波' '葫芦岛' '商洛'\n",
      " '淮北' '自贡' '双鸭山' '安庆' '六盘水' '广安' '沧州' '衡水' '芜湖' '楚雄彝族自治州' '许昌' '舟山' '松原'\n",
      " '齐齐哈尔' '文山壮族苗族自治州' '盘锦' '通辽' '本溪' '朔州' '七台河' '保山' '铜仁' '忻州' '承德' '宣城'\n",
      " '云浮' '雅安' '池州' '黄石' '吉林' '株洲' '开封' '上饶' '贵港' '大兴安岭' '临沧' '毕节' '铁岭' '驻马店'\n",
      " '大连' '石嘴山' '鹰潭' '湘西土家族苗族自治州' '崇左' '安顺' '延安' '呼伦贝尔' '济源' '武威' '莱芜'\n",
      " '大理白族自治州' '白城' '通化' '西双版纳傣族自治州' '锡林郭勒盟' '内江' '伊春' '庆阳' '湘潭' '铜川' '张掖'\n",
      " '防城港' '鹤壁' '辽阳' '石河子' '克孜勒苏柯尔克孜自治州' '昌吉回族自治州' '凉山' '遂宁' '亳州' '阿坝藏族羌族自治州'\n",
      " '拉萨' '博尔塔拉蒙古自治州' '巴音郭楞蒙古自治州' '临夏回族自治州' '金昌' '和田' '铜陵' '恩施' '嘉峪关' '鹤岗'\n",
      " '哈密' '甘孜藏族自治州' '兴安盟' '贺州' '辽源' '海西蒙古族藏族自治州' '甘南藏族自治州' '海北藏族自治州' '林芝' '昌都'\n",
      " '迪庆藏族自治州' '吐鲁番']\n"
     ]
    }
   ],
   "source": [
    "# 先将data 和 test 数据进行合并再进行清洗\n",
    "# 2 字符串的清洗\n",
    "# 定义清洗用的函数\n",
    "def deleteChars(s, c):\n",
    "    return s.replace(c, '')\n",
    "\n",
    "# 根据data.describe() 观察到UserInfo_2, UserInfo_4, UserInfo_8, UserInfo_20都是城市信息，在此做一个清洗工作\n",
    "clean_cols = ['UserInfo_2', 'UserInfo_4', 'UserInfo_8', 'UserInfo_20']\n",
    "for c in clean_cols:\n",
    "    data[c] = data[c].apply(deleteChars, c = '市')\n",
    "\n",
    "print(data['UserInfo_2'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Idx</th>\n",
       "      <th>UserInfo_1</th>\n",
       "      <th>UserInfo_2</th>\n",
       "      <th>UserInfo_3</th>\n",
       "      <th>UserInfo_4</th>\n",
       "      <th>WeblogInfo_2</th>\n",
       "      <th>WeblogInfo_4</th>\n",
       "      <th>WeblogInfo_5</th>\n",
       "      <th>WeblogInfo_6</th>\n",
       "      <th>WeblogInfo_7</th>\n",
       "      <th>...</th>\n",
       "      <th>SocialNetwork_11</th>\n",
       "      <th>SocialNetwork_12</th>\n",
       "      <th>SocialNetwork_13</th>\n",
       "      <th>SocialNetwork_14</th>\n",
       "      <th>SocialNetwork_15</th>\n",
       "      <th>SocialNetwork_16</th>\n",
       "      <th>SocialNetwork_17</th>\n",
       "      <th>target</th>\n",
       "      <th>ListingInfo</th>\n",
       "      <th>special_feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10001</td>\n",
       "      <td>1.000</td>\n",
       "      <td>深圳</td>\n",
       "      <td>4.000</td>\n",
       "      <td>深圳</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2014/3/5</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10002</td>\n",
       "      <td>1.000</td>\n",
       "      <td>温州</td>\n",
       "      <td>4.000</td>\n",
       "      <td>温州</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2014/2/26</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10003</td>\n",
       "      <td>1.000</td>\n",
       "      <td>宜昌</td>\n",
       "      <td>3.000</td>\n",
       "      <td>宜昌</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2014/2/28</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10008</td>\n",
       "      <td>1.000</td>\n",
       "      <td>吴忠</td>\n",
       "      <td>5.000</td>\n",
       "      <td>银川</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2014/2/27</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10011</td>\n",
       "      <td>1.000</td>\n",
       "      <td>绵阳</td>\n",
       "      <td>3.000</td>\n",
       "      <td>赤峰</td>\n",
       "      <td>0.000</td>\n",
       "      <td>13.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>13.000</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2014/2/24</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 224 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Idx  UserInfo_1 UserInfo_2  UserInfo_3 UserInfo_4  WeblogInfo_2  \\\n",
       "0  10001       1.000         深圳       4.000         深圳         1.000   \n",
       "1  10002       1.000         温州       4.000         温州         0.000   \n",
       "2  10003       1.000         宜昌       3.000         宜昌         0.000   \n",
       "5  10008       1.000         吴忠       5.000         银川         0.000   \n",
       "6  10011       1.000         绵阳       3.000         赤峰         0.000   \n",
       "\n",
       "   WeblogInfo_4  WeblogInfo_5  WeblogInfo_6  WeblogInfo_7  ...  \\\n",
       "0         1.000         1.000         1.000            14  ...   \n",
       "1         1.000         1.000         1.000            14  ...   \n",
       "2         2.000         2.000         2.000             9  ...   \n",
       "5         2.000         2.000         2.000             4  ...   \n",
       "6        13.000         1.000        13.000            15  ...   \n",
       "\n",
       "   SocialNetwork_11  SocialNetwork_12  SocialNetwork_13  SocialNetwork_14  \\\n",
       "0                -1                 0                 0                 0   \n",
       "1                -1                 0                 0                 0   \n",
       "2                -1                -1                 1                 0   \n",
       "5                -1                -1                 0                 0   \n",
       "6                -1                 0                 1                 0   \n",
       "\n",
       "   SocialNetwork_15  SocialNetwork_16  SocialNetwork_17  target  ListingInfo  \\\n",
       "0                 0                 0                 1       0     2014/3/5   \n",
       "1                 0                 0                 2       0    2014/2/26   \n",
       "2                 0                 0                 0       0    2014/2/28   \n",
       "5                 0                 0                 0       0    2014/2/27   \n",
       "6                 0                 0                 1       1    2014/2/24   \n",
       "\n",
       "   special_feature  \n",
       "0            1.000  \n",
       "1            1.000  \n",
       "2            1.000  \n",
       "5            0.000  \n",
       "6            0.000  \n",
       "\n",
       "[5 rows x 224 columns]"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. 衍生特征\n",
    "# 对于UserInfo_2 和 UserInfo_4 相等的列增加一个新的feature\n",
    "data['special_feature'] = pd.Series(np.zeros(len(data)))\n",
    "data.loc[data['UserInfo_2'] == data['UserInfo_4'],'special_feature'] = 1\n",
    "data.loc[data['UserInfo_2'] != data['UserInfo_4'],'special_feature'] = 0\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "深圳           47\n",
       "成都           37\n",
       "广州           34\n",
       "重庆           26\n",
       "苏州           24\n",
       "上海           24\n",
       "东莞           22\n",
       "临沂           21\n",
       "潍坊           20\n",
       "青岛           20\n",
       "淄博           19\n",
       "南京           18\n",
       "石家庄          18\n",
       "温州           18\n",
       "武汉           18\n",
       "北京           17\n",
       "菏泽           17\n",
       "佛山           17\n",
       "烟台           17\n",
       "长沙           17\n",
       "泉州           17\n",
       "宜昌           16\n",
       "厦门           16\n",
       "中山           15\n",
       "盐城           15\n",
       "济南           13\n",
       "杭州           13\n",
       "宁波           13\n",
       "聊城           13\n",
       "哈尔滨          12\n",
       "             ..\n",
       "葫芦岛           1\n",
       "乌兰察布盟         1\n",
       "张家界           1\n",
       "乌海            1\n",
       "阳江            1\n",
       "平顶山           1\n",
       "蚌埠            1\n",
       "石嘴山           1\n",
       "四平            1\n",
       "巴音郭楞蒙古自治州     1\n",
       "防城港           1\n",
       "文山壮族苗族自治州     1\n",
       "天水            1\n",
       "白城            1\n",
       "乌鲁木齐          1\n",
       "临夏回族自治州       1\n",
       "阿克苏           1\n",
       "黄石            1\n",
       "黑河            1\n",
       "大同            1\n",
       "保山            1\n",
       "钦州            1\n",
       "黔西南           1\n",
       "广安            1\n",
       "吴忠            1\n",
       "抚州            1\n",
       "酒泉            1\n",
       "金昌            1\n",
       "营口            1\n",
       "内江            1\n",
       "Name: UserInfo_2, Length: 273, dtype: int64"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = data.select_dtypes(include=['object']).columns.tolist()\n",
    "data[columns].describe()\n",
    "data.loc[data['target'] == 1]['UserInfo_2'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "其他     11930\n",
      "深圳       510\n",
      "广州       471\n",
      "上海       429\n",
      "重庆       376\n",
      "北京       371\n",
      "温州       342\n",
      "泉州       337\n",
      "东莞       312\n",
      "成都       304\n",
      "苏州       260\n",
      "金华       229\n",
      "杭州       225\n",
      "郑州       202\n",
      "福州       201\n",
      "长沙       195\n",
      "武汉       195\n",
      "厦门       179\n",
      "台州       173\n",
      "青岛       173\n",
      "临沂       171\n",
      "赣州       166\n",
      "佛山       162\n",
      "宁波       161\n",
      "南京       156\n",
      "潍坊       155\n",
      "昆明       153\n",
      "中山       152\n",
      "海口       144\n",
      "天津       133\n",
      "盐城       133\n",
      "济南       132\n",
      "石家庄      132\n",
      "哈尔滨      126\n",
      "徐州       124\n",
      "宜昌       120\n",
      "聊城       119\n",
      "邯郸       117\n",
      "滨州       101\n",
      "烟台        95\n",
      "安阳        90\n",
      "临汾        90\n",
      "淄博        89\n",
      "菏泽        83\n",
      "大连        83\n",
      "泰州        78\n",
      "绵阳        77\n",
      "珠海        73\n",
      "河源        62\n",
      "丹东        51\n",
      "永州        46\n",
      "Name: UserInfo_2, dtype: int64\n",
      "其他     11529\n",
      "深圳       579\n",
      "广州       543\n",
      "上海       474\n",
      "北京       451\n",
      "重庆       382\n",
      "成都       351\n",
      "泉州       332\n",
      "温州       328\n",
      "东莞       318\n",
      "杭州       254\n",
      "金华       251\n",
      "苏州       249\n",
      "郑州       248\n",
      "武汉       237\n",
      "长沙       210\n",
      "福州       201\n",
      "青岛       193\n",
      "厦门       181\n",
      "昆明       176\n",
      "宁波       170\n",
      "中山       165\n",
      "台州       165\n",
      "临沂       162\n",
      "佛山       159\n",
      "南京       153\n",
      "赣州       152\n",
      "济南       148\n",
      "天津       145\n",
      "潍坊       145\n",
      "石家庄      136\n",
      "海口       132\n",
      "哈尔滨      130\n",
      "徐州       126\n",
      "盐城       119\n",
      "聊城       118\n",
      "邯郸       111\n",
      "宜昌       109\n",
      "滨州        95\n",
      "烟台        93\n",
      "大连        90\n",
      "安阳        84\n",
      "临汾        84\n",
      "淄博        77\n",
      "菏泽        76\n",
      "珠海        74\n",
      "绵阳        71\n",
      "泰州        70\n",
      "河源        51\n",
      "丹东        48\n",
      "永州        43\n",
      "Name: UserInfo_4, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 4 二值化处理\n",
    "# 计算出逾期率最高的K个城市，这里K = 4\n",
    "K = 50\n",
    "# 通过data.describe()观察出需要做二值化的columns，因为存在大量的类别数据。\n",
    "bin_cols = ['UserInfo_2', 'UserInfo_4', 'UserInfo_8', 'UserInfo_20']\n",
    "for c in bin_cols:\n",
    "    cities = data.loc[data['target'] == 1][c].value_counts().index.tolist()\n",
    "    # 将前K个城市的值保留，其他城市都用其他代替\n",
    "    data.loc[~data[c].isin(cities1[:K]), c] = '其他'\n",
    "\n",
    "# cities1 = data.loc[data['target'] == 1]['UserInfo_2'].value_counts().index.tolist()\n",
    "# cities2 = data.loc[data['target'] == 1]['UserInfo_4'].value_counts().index.tolist()\n",
    "\n",
    "# data.loc[~data['UserInfo_2'].isin(cities1[:K]), 'UserInfo_2'] = '其他'\n",
    "# data.loc[~data['UserInfo_4'].isin(cities2[:K]), 'UserInfo_4'] = '其他'\n",
    "print(data['UserInfo_2'].value_counts())\n",
    "print(data['UserInfo_4'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['UserInfo_2', 'UserInfo_4', 'UserInfo_7', 'UserInfo_8', 'UserInfo_9', 'UserInfo_19', 'UserInfo_20', 'UserInfo_22', 'UserInfo_23', 'Education_Info2', 'Education_Info3', 'Education_Info4', 'Education_Info6', 'Education_Info7', 'Education_Info8', 'WeblogInfo_19', 'WeblogInfo_20', 'WeblogInfo_21']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserInfo_2</th>\n",
       "      <th>UserInfo_4</th>\n",
       "      <th>UserInfo_7</th>\n",
       "      <th>UserInfo_8</th>\n",
       "      <th>UserInfo_9</th>\n",
       "      <th>UserInfo_19</th>\n",
       "      <th>UserInfo_20</th>\n",
       "      <th>UserInfo_22</th>\n",
       "      <th>UserInfo_23</th>\n",
       "      <th>Education_Info2</th>\n",
       "      <th>Education_Info3</th>\n",
       "      <th>Education_Info4</th>\n",
       "      <th>Education_Info6</th>\n",
       "      <th>Education_Info7</th>\n",
       "      <th>Education_Info8</th>\n",
       "      <th>WeblogInfo_19</th>\n",
       "      <th>WeblogInfo_20</th>\n",
       "      <th>WeblogInfo_21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20988</td>\n",
       "      <td>20988</td>\n",
       "      <td>20988</td>\n",
       "      <td>20988</td>\n",
       "      <td>20988</td>\n",
       "      <td>20988</td>\n",
       "      <td>20988</td>\n",
       "      <td>20988</td>\n",
       "      <td>20988</td>\n",
       "      <td>20988</td>\n",
       "      <td>20988</td>\n",
       "      <td>20988</td>\n",
       "      <td>20988</td>\n",
       "      <td>20988</td>\n",
       "      <td>20988</td>\n",
       "      <td>20988</td>\n",
       "      <td>20988</td>\n",
       "      <td>20988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>32</td>\n",
       "      <td>51</td>\n",
       "      <td>7</td>\n",
       "      <td>31</td>\n",
       "      <td>46</td>\n",
       "      <td>7</td>\n",
       "      <td>26</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>36</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>其他</td>\n",
       "      <td>其他</td>\n",
       "      <td>不详</td>\n",
       "      <td>其他</td>\n",
       "      <td>中国移动</td>\n",
       "      <td>山东省</td>\n",
       "      <td>其他</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>I</td>\n",
       "      <td>I5</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>11930</td>\n",
       "      <td>11529</td>\n",
       "      <td>2920</td>\n",
       "      <td>12625</td>\n",
       "      <td>10868</td>\n",
       "      <td>1736</td>\n",
       "      <td>16305</td>\n",
       "      <td>19276</td>\n",
       "      <td>19276</td>\n",
       "      <td>19502</td>\n",
       "      <td>19502</td>\n",
       "      <td>19502</td>\n",
       "      <td>20235</td>\n",
       "      <td>20235</td>\n",
       "      <td>20235</td>\n",
       "      <td>17947</td>\n",
       "      <td>10773</td>\n",
       "      <td>17796</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       UserInfo_2 UserInfo_4 UserInfo_7 UserInfo_8 UserInfo_9 UserInfo_19  \\\n",
       "count       20988      20988      20988      20988      20988       20988   \n",
       "unique         51         51         32         51          7          31   \n",
       "top            其他         其他         不详         其他      中国移动          山东省   \n",
       "freq        11930      11529       2920      12625      10868        1736   \n",
       "\n",
       "       UserInfo_20 UserInfo_22 UserInfo_23 Education_Info2 Education_Info3  \\\n",
       "count        20988       20988       20988           20988           20988   \n",
       "unique          46           7          26               7               3   \n",
       "top             其他           D           D               E               E   \n",
       "freq         16305       19276       19276           19502           19502   \n",
       "\n",
       "       Education_Info4 Education_Info6 Education_Info7 Education_Info8  \\\n",
       "count            20988           20988           20988           20988   \n",
       "unique               6               6               2               7   \n",
       "top                  E               E               E               E   \n",
       "freq             19502           20235           20235           20235   \n",
       "\n",
       "       WeblogInfo_19 WeblogInfo_20 WeblogInfo_21  \n",
       "count          20988         20988         20988  \n",
       "unique             7            36             4  \n",
       "top                I            I5             D  \n",
       "freq           17947         10773         17796  "
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5 特征独热编码\n",
    "# UserInfo_24是地址信息且数据样本极大，这里做删除操作。\n",
    "data.drop('UserInfo_24', axis = 1, inplace = True)\n",
    "# ListingInfo 是时间信息与训练无关也做删除操作\n",
    "data.drop('ListingInfo', axis = 1, inplace = True)\n",
    "\n",
    "# 选出所有‘类别’行数据进行独热编码\n",
    "columns = data.select_dtypes(include=['object']).columns.tolist()\n",
    "print(columns)\n",
    "data[columns].describe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['UserInfo_2_其他', 'UserInfo_4_其他', 'UserInfo_8_其他', 'UserInfo_20_其他']\n",
      "(20988, 580)\n"
     ]
    }
   ],
   "source": [
    "# 对数据就进行独热编码并删除之前加入‘其他’的列\n",
    "data = pd.get_dummies(data, columns = columns,  prefix_sep=\"_\", dummy_na = False, drop_first = False)\n",
    "# 删除带 ‘其他’的列\n",
    "# data.head()\n",
    "dropList = [x + '_其他' for x in bin_cols]\n",
    "print(dropList)\n",
    "data.drop(columns = dropList, axis=1, inplace=True)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### 2. 特征选择\n",
    "200多个特征里可能有效的特征不会很多。在这里做特征选择相关的工作。 在特征选择这一块请使用```树```模型。 比如sklearn自带的特征选择模块（https://scikit-learn.org/stable/modules/feature_selection.html）， 或者直接使用XGBoost等模型来直接选择。 这些模型训练好之后你可以直接通过```feature_importance_values```属性来获取。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3264, 580)\n"
     ]
    }
   ],
   "source": [
    "# 进行下采样\n",
    "positive_data = data[data['target'] == 1]  # 正样本\n",
    "negative_data = data[data['target'] == 0]  # 负样本\n",
    "\n",
    "lower_data = negative_data.sample(n = len(positive_data), replace = False, random_state=42, axis = 0)\n",
    "data_resample = pd.concat([positive_data, lower_data])\n",
    "print(data_resample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2611, 579) (2611,) (653, 579) (2611,)\n"
     ]
    }
   ],
   "source": [
    "# 进行训练集和测试集的分割\n",
    "feature_names = np.array(data_resample.columns[data_resample.columns != 'target'].tolist())\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data_resample[feature_names].values, \n",
    "    data_resample['target'].values,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.00000000e-04 3.16227766e-04 1.00000000e-03 3.16227766e-03\n",
      " 1.00000000e-02 3.16227766e-02 1.00000000e-01 3.16227766e-01\n",
      " 1.00000000e+00 3.16227766e+00 1.00000000e+01]\n",
      "Fitting 5 folds for each of 11 candidates, totalling 55 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed: 25.5min\n",
      "[Parallel(n_jobs=-1)]: Done  55 out of  55 | elapsed: 48.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best f1-score:  0.01534462465680796\n",
      "Best parameters:  {'fs__estimator__C': 0.001}\n"
     ]
    }
   ],
   "source": [
    "# 1)采用逻辑回归L1作为特征选择的base\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import f1_score\n",
    "#from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "params_c = np.logspace(-4, 1, 11)\n",
    "print(params_c)\n",
    "\n",
    "# 使用逻辑回归+selectfromModel进行特征筛选\n",
    "pipe = Pipeline([\n",
    "    ('fs', SelectFromModel(estimator=LogisticRegression(penalty='l1'))),\n",
    "    ('clf', LogisticRegression())\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'fs__estimator__C': params_c,\n",
    "}\n",
    "\n",
    "kf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 42)\n",
    "model = GridSearchCV(pipe, parameters, cv = kf, n_jobs= -1, verbose = 1, scoring = 'f1') # f1 socring for binary classifier\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print('Best f1-score: ', model.best_score_)    \n",
    "best_parameters = model.best_params_\n",
    "print('Best parameters: ', best_parameters) \n",
    "\n",
    "# 求出c_best\n",
    "c_best = best_parameters['fs__estimator__C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过c_best值，重新在整个X_train里做训练，并选出特征。\n",
    "c_best = 0.001\n",
    "lr_clf = LogisticRegression(penalty='l1', C=c_best)\n",
    "lr_clf.fit(X_train, y_train) # 在整个训练数据重新训练\n",
    "\n",
    "select_model = SelectFromModel(lr_clf, prefit=True)\n",
    "selected_features = select_model.get_support()  # 被选出来的特征\n",
    "\n",
    "# 重新构造feature_names\n",
    "feature_names = feature_names[selected_features]\n",
    "\n",
    "# 重新构造训练数据和测试数据\n",
    "X_train1 = X_train[:, selected_features]\n",
    "X_test1 = X_test[:, selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best roc_auc:  0.6436987843172806\n",
      "Best parameters: {'clf__C': 0.03162277660168379}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    1.3s finished\n"
     ]
    }
   ],
   "source": [
    "# 使用选择后的逻辑回归训练比计算score\n",
    "from sklearn.metrics import roc_auc_score \n",
    "\n",
    "params_c = np.logspace(-5,2,5) # 也可以自行定义一个范围\n",
    "\n",
    "# TODO: 实现逻辑回归 + L2正则， 利用GrisSearchCV\n",
    "pipe = Pipeline([\n",
    "    ('clf', LogisticRegression(penalty='l2', solver='lbfgs'))\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'clf__C': params_c\n",
    "}\n",
    "\n",
    "kf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 42)\n",
    "model = GridSearchCV(pipe, parameters, cv = kf, n_jobs= -1, verbose = 1, scoring = 'roc_auc') # roc_auc\n",
    "model.fit(X_train1, y_train)\n",
    "\n",
    "# 输出最好的参数 \n",
    "print('Best roc_auc: ', model.best_score_)\n",
    "print('Best parameters:', model.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5685275599457096\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test1)\n",
    "print(roc_auc_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc: 66.30%\n",
      "Thresh=0.000, n=135, Accuracy: 66.30%\n",
      "Thresh=0.003, n=113, Accuracy: 66.96%\n",
      "Thresh=0.003, n=111, Accuracy: 67.27%\n",
      "Thresh=0.004, n=109, Accuracy: 68.29%\n",
      "Thresh=0.004, n=109, Accuracy: 68.29%\n",
      "Thresh=0.004, n=102, Accuracy: 67.18%\n",
      "Thresh=0.005, n=101, Accuracy: 67.12%\n",
      "Thresh=0.005, n=100, Accuracy: 67.04%\n",
      "Thresh=0.005, n=98, Accuracy: 67.51%\n",
      "Thresh=0.005, n=95, Accuracy: 66.82%\n",
      "Thresh=0.006, n=90, Accuracy: 66.96%\n",
      "Thresh=0.006, n=85, Accuracy: 67.18%\n",
      "Thresh=0.006, n=85, Accuracy: 67.18%\n",
      "Thresh=0.006, n=82, Accuracy: 66.55%\n",
      "Thresh=0.007, n=79, Accuracy: 66.47%\n",
      "Thresh=0.007, n=77, Accuracy: 65.28%\n",
      "Thresh=0.007, n=77, Accuracy: 65.28%\n",
      "Thresh=0.007, n=73, Accuracy: 65.67%\n",
      "Thresh=0.007, n=73, Accuracy: 65.67%\n",
      "Thresh=0.008, n=71, Accuracy: 65.64%\n",
      "Thresh=0.009, n=70, Accuracy: 66.24%\n",
      "Thresh=0.009, n=63, Accuracy: 66.20%\n",
      "Thresh=0.010, n=56, Accuracy: 66.04%\n",
      "Thresh=0.011, n=53, Accuracy: 66.61%\n",
      "Thresh=0.012, n=49, Accuracy: 66.12%\n",
      "Thresh=0.013, n=46, Accuracy: 65.96%\n",
      "Thresh=0.015, n=41, Accuracy: 65.12%\n"
     ]
    }
   ],
   "source": [
    "# 2）使用XGBoost来做特征选择\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "result_roc_auc = roc_auc_score(y_test, y_pred)\n",
    "print(\"roc_auc: %.2f%%\" % (result_roc_auc * 100.0))\n",
    "# Fit model using each importance as a threshold\n",
    "# print(model.feature_importances_)\n",
    "thresholds = np.sort(model.feature_importances_)\n",
    "#print(thresholds)\n",
    "best_roc_auc = 0\n",
    "best_thresh = 0\n",
    "X_train2 = None\n",
    "X_test2 = None\n",
    "select_X_train = X_train\n",
    "select_X_test = X_test\n",
    "nCount = 0\n",
    "for thresh in thresholds:\n",
    "    # select features using threshold\n",
    "    if thresh < 10e-5:\n",
    "        continue\n",
    "    select_model = SelectFromModel(model, threshold=thresh, prefit = True)\n",
    "    select_X_train = select_model.transform(select_X_train)\n",
    "    \n",
    "    # train model\n",
    "    model = XGBClassifier()\n",
    "    model.fit(select_X_train, y_train)\n",
    "    # eval model\n",
    "    select_X_test = select_model.transform(select_X_test)\n",
    "    y_pred = model.predict(select_X_test)\n",
    "    current_roc_auc = roc_auc_score(y_test, y_pred)\n",
    "    if nCount % 5 == 0:\n",
    "        print(\"Thresh=%.3f, n=%d, Accuracy: %.2f%%\" % (thresh, select_X_train.shape[1], current_roc_auc*100.0))\n",
    "    nCount += 1\n",
    "    # select the best features\n",
    "    if current_roc_auc > best_roc_auc:\n",
    "        best_roc_auc = current_roc_auc\n",
    "        best_thresh = thresh\n",
    "        X_train2 = select_X_train\n",
    "        X_test2 = select_X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2611, 107) (653, 107)\n"
     ]
    }
   ],
   "source": [
    "# X_train2, X_test2就是选出来的最好特征\n",
    "print(X_train2.shape, X_test2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### 3. XGBoost来训练风控模型，结果以AUC为准\n",
    "https://github.com/dmlc/xgboost   这是XGBoost library具体的地址, 具有详细的文档。 https://pypi.org/project/xgboost/ 里有安装的步骤。 试着去调一下它的超参数，使得得到最好的效果。 一定要注意不需要使用测试数据来训练。 最终的结果以测试数据上的AUC为标准。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    6.3s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    7.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best roc_auc_score:  0.7297296310319055\n",
      "Best parameters:  {'clf__eta': 0.01}\n"
     ]
    }
   ],
   "source": [
    "# data_test = pd.read_csv('data/Test/PPD_Master_GBK_2_Test_Set.csv',encoding='gb18030')\n",
    "clf = XGBClassifier()\n",
    "# 1)greedy approach adjust learning rate\n",
    "parameters = {\n",
    "    # learning rate\n",
    "    'clf__eta': np.logspace(-2, 1, 10)\n",
    "}\n",
    "\n",
    "kf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 42)\n",
    "model = GridSearchCV(clf, parameters, cv = kf, n_jobs= -1, verbose = 1, scoring = 'roc_auc') # roc_auc_score\n",
    "model.fit(X_train2, y_train)\n",
    "\n",
    "print('Best roc_auc_score: ', model.best_score_)    \n",
    "best_parameters = model.best_params_\n",
    "print('Best parameters: ', best_parameters)\n",
    "\n",
    "# 求出eta best\n",
    "eta_best = best_parameters['clf__eta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    6.3s\n",
      "[Parallel(n_jobs=-1)]: Done 125 out of 125 | elapsed:   16.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best roc_auc_score:  0.7297296310319055\n",
      "Best parameters:  {'clf__max_depth': 3, 'clf__min_child_weight': 1}\n"
     ]
    }
   ],
   "source": [
    "# ）greedy approach adjust for\n",
    "# max_depth and min_child_weight \n",
    "clf = XGBClassifier(eta = eta_best)\n",
    "parameters = {\n",
    "    'clf__max_depth': [3, 5, 7, 9, 10],\n",
    "    'clf__min_child_weight': [1, 2, 4, 8, 16]\n",
    "}\n",
    "\n",
    "kf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 42)\n",
    "model = GridSearchCV(clf, parameters, cv = kf, n_jobs= -1, verbose = 1, scoring = 'roc_auc') # roc_auc_score\n",
    "model.fit(X_train2, y_train)\n",
    "\n",
    "print('Best roc_auc_score: ', model.best_score_)    \n",
    "best_parameters = model.best_params_\n",
    "print('Best parameters: ', best_parameters)\n",
    "\n",
    "# 求出best max_depth 和 min_child_weight\n",
    "max_depth_best = best_parameters['clf__max_depth']\n",
    "min_child_weight = best_parameters['clf__min_child_weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    5.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best roc_auc_score:  0.7297296310319055\n",
      "Best parameters:  {'clf__n_estimators': 400}\n"
     ]
    }
   ],
   "source": [
    "# greedy approach for n_estimator\n",
    "clf = XGBClassifier(eta = eta_best, max_depth = max_depth_best, min_child_weight = min_child_weight)\n",
    "parameters = {\n",
    "    'clf__n_estimators': [400, 500, 600, 700, 800]\n",
    "}\n",
    "kf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 42)\n",
    "model = GridSearchCV(clf, parameters, cv = kf, n_jobs= -1, verbose = 1, scoring = 'roc_auc') # roc_auc_score\n",
    "model.fit(X_train2, y_train)\n",
    "\n",
    "print('Best roc_auc_score: ', model.best_score_)    \n",
    "best_parameters = model.best_params_\n",
    "print('Best parameters: ', best_parameters)\n",
    "\n",
    "# 求出best n_estimators\n",
    "n_estimator_best = best_parameters['clf__n_estimators']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:   13.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best roc_auc_score:  0.72417734324015\n",
      "Best parameters:  {'clf__gamma': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# greedy approach for gamma\n",
    "# {'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]}\n",
    "clf = XGBClassifier(eta = eta_best, \n",
    "                    max_depth = max_depth_best, \n",
    "                    min_child_weight = min_child_weight, \n",
    "                    n_estimators = n_estimator_best)\n",
    "parameters = {\n",
    "    'clf__gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n",
    "}\n",
    "kf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 42)\n",
    "model = GridSearchCV(clf, parameters, cv = kf, n_jobs= -1, verbose = 1, scoring = 'roc_auc') # roc_auc_score\n",
    "model.fit(X_train2, y_train)\n",
    "\n",
    "print('Best roc_auc_score: ', model.best_score_)    \n",
    "best_parameters = model.best_params_\n",
    "print('Best parameters: ', best_parameters)\n",
    "\n",
    "# 求出best gamma\n",
    "gamma_best = best_parameters['clf__gamma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   17.5s\n",
      "[Parallel(n_jobs=-1)]: Done  80 out of  80 | elapsed:   35.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best roc_auc_score:  0.7280011885040995\n",
      "Best parameters:  {'clf__colsample_bytree': 0.4, 'clf__subsample': 0.4}\n"
     ]
    }
   ],
   "source": [
    "# greedy approach\n",
    "clf = XGBClassifier(eta = eta_best, \n",
    "                    max_depth = max_depth_best, \n",
    "                    min_child_weight = min_child_weight, \n",
    "                    n_estimators = n_estimator_best,\n",
    "                    gamma = gamma_best)\n",
    "parameters = {\n",
    "    'clf__subsample': [0.4, 0.5, 0.6, 0.7],\n",
    "    'clf__colsample_bytree': [0.4, 0.5, 0.6, 0.7]\n",
    "}\n",
    "kf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 42)\n",
    "model = GridSearchCV(clf, parameters, cv = kf, n_jobs= -1, verbose = 1, scoring = 'roc_auc') # roc_auc_score\n",
    "model.fit(X_train2, y_train)\n",
    "\n",
    "print('Best roc_auc_score: ', model.best_score_)    \n",
    "best_parameters = model.best_params_\n",
    "print('Best parameters: ', best_parameters)\n",
    "\n",
    "# 求出best gamma\n",
    "subsample_best = best_parameters['clf__subsample']\n",
    "colsample_bytree_best = best_parameters['clf__colsample_bytree']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    7.3s\n",
      "[Parallel(n_jobs=-1)]: Done 125 out of 125 | elapsed:   23.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best roc_auc_score:  0.7284200787357374\n",
      "Best parameters:  {'clf__reg_alpha': 0.05, 'clf__reg_lambda': 0.05}\n"
     ]
    }
   ],
   "source": [
    "# greedy approach\n",
    "clf = XGBClassifier(eta = eta_best, \n",
    "                    max_depth = max_depth_best, \n",
    "                    min_child_weight = min_child_weight, \n",
    "                    n_estimators = n_estimator_best,\n",
    "                    gamma = gamma_best,\n",
    "                    subsample = subsample_best,\n",
    "                    colsample_bytree = colsample_bytree_best)\n",
    "parameters = {\n",
    "    'clf__reg_alpha': [0.05, 0.1, 1, 2, 3], \n",
    "    'clf__reg_lambda': [0.05, 0.1, 1, 2, 3]\n",
    "}\n",
    "\n",
    "kf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 42)\n",
    "model = GridSearchCV(clf, parameters, cv = kf, n_jobs= -1, verbose = 1, scoring = 'roc_auc') # roc_auc_score\n",
    "model.fit(X_train2, y_train)\n",
    "\n",
    "print('Best roc_auc_score: ', model.best_score_)    \n",
    "best_parameters = model.best_params_\n",
    "print('Best parameters: ', best_parameters)\n",
    "\n",
    "# 求出best gamma\n",
    "reg_alpha_best = best_parameters['clf__reg_alpha']\n",
    "reg_lambda_best = best_parameters['clf__reg_lambda']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc_score in the test data set is: 0.6536\n"
     ]
    }
   ],
   "source": [
    "# calculate the auc rate for test data\n",
    "predictions = model.predict(X_test2)\n",
    "print(\"roc_auc_score in the test data set is: %.4f\" % roc_auc_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
