{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer with ngram model\n",
    "### 1. text preprocessing with the simpleified-chinese text corpus\n",
    "### 2. test the perplexity with the text corpus with unigram, bigram, trigram\n",
    "### 3. implementation the viterbi algorithm with unigram, bigram, trigram.  \n",
    "corpus from https://pan.baidu.com/s/1YJkY48u6DN1HUirb-_7cTw pw: smkk  \n",
    "### sourcefile:   \n",
    "language_Model.py  \n",
    "preprocess.py\n",
    "### corpus file:\n",
    "msr_training.utf8  \n",
    "msr_test.utf8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section1\n",
    "load the data set and do some text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "def load_data(data_dir):\n",
    "    \"\"\"Load train and test corpora from a directory.\n",
    "    Directory must contain two files: train.txt and test.txt.\n",
    "    Newlines will be stripped out. \n",
    "    Args:\n",
    "        data_dir (Path) -- pathlib.Path of the directory to use. \n",
    "    Returns:\n",
    "        The train and test sets, as lists of sentences.\n",
    "    \"\"\"\n",
    "    train_path = data_dir.joinpath('msr_training.utf8').absolute().as_posix()\n",
    "    test_path  = data_dir.joinpath('msr_test.utf8').absolute().as_posix()\n",
    "\n",
    "    with open(train_path, 'r', encoding = 'utf8') as f:\n",
    "        train = [l.strip() for l in f.readlines()]\n",
    "    with open(test_path, 'r', encoding = 'utf8') as f:\n",
    "        test = [l.strip() for l in f.readlines()]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from the current directory.\n",
    "train, test = load_data(Path('.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "SOS = \"<s> \"\n",
    "EOS = \"</s>\"\n",
    "UNK = \"<UNK>\"\n",
    "# init the toktoktokenizer\n",
    "toktok = nltk.tokenize.ToktokTokenizer()\n",
    "word_tokenize = toktok.tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add sentence token\n",
    "def add_sentence_tokens(sentences, n):\n",
    "    \"\"\"Wrap each sentence in SOS and EOS tokens.\n",
    "    For n >= 2, n-1 SOS tokens are added, otherwise only one is added.\n",
    "    Args:\n",
    "        sentences (list of str): the sentences to wrap.\n",
    "        n (int): order of the n-gram model which will use these sentences.\n",
    "    Returns:\n",
    "        List of sentences with SOS and EOS tokens wrapped around them.\n",
    "    \"\"\"\n",
    "    sos = SOS * (n-1) if n > 1 else SOS\n",
    "    return ['{}{} {}'.format(sos, s, EOS) for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_singletons(tokens):\n",
    "    \"\"\"Replace tokens which appear only once in the corpus with <UNK>.\n",
    "    \n",
    "    Args:\n",
    "        tokens (list of str): the tokens comprising the corpus.\n",
    "    Returns:\n",
    "        The same list of tokens with each singleton replaced by <UNK>.\n",
    "    \n",
    "    \"\"\"\n",
    "    vocab = nltk.FreqDist(tokens)\n",
    "    return [token if vocab[token] > 1 else UNK for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_clean(sentences, pattern = ''):\n",
    "    \"\"\"clean the characters in the input sentences.\n",
    "    \n",
    "    Args:\n",
    "        sentences (list of str): the sentences to preprocess.\n",
    "        pattern (str): the regular expression to be filted from the sentence.\n",
    "    Returns:\n",
    "        The cleaned sentences, filted with the specific pattern.\n",
    "    \"\"\"\n",
    "    # by default we will filt the \\n characters.\n",
    "    cleaner = lambda x : x.replace('\\n', '')\n",
    "    if pattern != '':\n",
    "        pa = re.compile(pattern)\n",
    "        pattern_filter = lambda x : re.sub(pa, '', x)\n",
    "        return [pattern_filter(cleaner(sent)) for sent in sentences]\n",
    "    else:\n",
    "        return [cleaner(sent) for sent in sentences]\n",
    "\n",
    "def preprocess(sentences, n):\n",
    "    \"\"\"Add SOS/EOS/UNK tokens to given sentences and tokenize.\n",
    "    Args:\n",
    "        sentences (list of str): the sentences to preprocess.\n",
    "        n (int): order of the n-gram model which will use these sentences.\n",
    "    Returns:\n",
    "        The preprocessed sentences, tokenized by words.\n",
    "    \"\"\"\n",
    "    sentences = add_sentence_tokens(sentences, n)\n",
    "    sentences = sentence_clean(sentences)\n",
    "    tokens = word_tokenize(' '.join(sentences))\n",
    "    tokens = replace_singletons(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '“', '<UNK>', '<UNK>', '知识', '<UNK>', '<UNK>', '知识', '<UNK>', '<UNK>', '，', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '</s>', '<s>', '“', '<UNK>', '<UNK>', '<UNK>', '<UNK>', '的', '<UNK>', '<UNK>', '，', '<UNK>', '<UNK>', '一个', '<UNK>', '的', '父亲', '，', '<UNK>', '<UNK>', '一个', '<UNK>', '<UNK>', '的', '父亲', '<UNK>', '</s>']\n"
     ]
    }
   ],
   "source": [
    "sentences = ['“  一点  外语  知识  、  数理化  知识  也  没有  ，  还  攀  什么  高峰  ？', \n",
    "                '“  在  我们  做  子女  的  眼  中  ，  他  是  一个  严厉  的  父亲  ，  同时  又是  一个  充满  爱心  的  父亲  。']\n",
    "print(preprocess(sentences, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section2  \n",
    "1) Train the ngram model and evaluate the model's basic function  \n",
    "2) Test the perplexity of the ngram model with the test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Language_Model import LanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 3-gram model...\n",
      "Vocabulary size: 46811\n"
     ]
    }
   ],
   "source": [
    "# create the ngram model\n",
    "print(\"Loading {}-gram model...\".format(3))\n",
    "lm = LanguageModel(train, 3, laplace=0.1)\n",
    "print(\"Vocabulary size: {}\".format(len(lm.vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model perplexity: 61.735\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test the perplexity\n",
    "perplexity = lm.perplexity(test)\n",
    "print(\"Model perplexity: {:.3f}\".format(perplexity))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1-gram model...\n",
      "Vocabulary size: 46811\n",
      "Model perplexity: 43.386\n",
      "\n",
      "Training 2-gram model...\n",
      "Vocabulary size: 46811\n",
      "Model perplexity: 29.768\n",
      "\n",
      "Training 3-gram model...\n",
      "Vocabulary size: 46811\n",
      "Model perplexity: 61.735\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we are going to train the unigram, bigram, and trigram with and output the perplexity.\n",
    "for i in range(1, 4):\n",
    "    print(\"Training {}-gram model...\".format(i))\n",
    "    mode = LanguageModel(train, i, laplace=0.1)\n",
    "    print(\"Vocabulary size: {}\".format(len(mode.vocab)))\n",
    "    perplexity = mode.perplexity(test)\n",
    "    print(\"Model perplexity: {:.3f}\".format(perplexity))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.230666007379586\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "testGram = ['风险投', '的', '目的']\n",
    "print(lm.getScore(testGram))\n",
    "print('<s>' in lm.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# test unigram count\n",
    "print(lm._convert_oov(['风险投']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section3\n",
    "\n",
    "Implementation the viterbi algorithm with the n-gram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1], [-1, 0], [0, 1], [1, 2], [2, 3], [4], [4, 5]]\n"
     ]
    }
   ],
   "source": [
    "def generateGraph(input_str, dic_words):\n",
    "    \"\"\" Generate the link graph of the input sentence base on the dictionary.\n",
    "    Args:\n",
    "        input_str (str): the sentence to preprocess.\n",
    "        dic_words (dict): the dictionary stored the words.\n",
    "    Returns:\n",
    "        The generated graph structure. each element of the list store the info\n",
    "        with respected to the inbound node.\n",
    "    \"\"\"\n",
    "    # init the graph\n",
    "    graph = [[] for i in range(len(input_str))]\n",
    "    for i in range(len(input_str)):\n",
    "        for j in range(i, len(input_str)):\n",
    "            substr = input_str[i:(j + 1)]\n",
    "            if substr in dic_words:\n",
    "                # add a inbound edge and weight for the graph\n",
    "                graph[j].append(i - 1)\n",
    "            elif j == i:\n",
    "                # default weight for single character\n",
    "                graph[j].append(i - 1)\n",
    "    return graph\n",
    "# test code\n",
    "print(generateGraph(\"经常有意见分歧\", lm.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'a', 'b']\n"
     ]
    }
   ],
   "source": [
    "# ngram padding function\n",
    "def generateNGram(input_list, n):\n",
    "    \"\"\" Generate the nGram from the input list.\n",
    "    Args:\n",
    "        input_list (str): the list of the adjacent words.\n",
    "        n (int): the output level of the gram\n",
    "    Returns:\n",
    "        The generated n-tuple of the ngram model\n",
    "    \"\"\"\n",
    "    # if the input len less or equal than n. we do an index slice.\n",
    "    if len(input_list) >= n:\n",
    "        return input_list[-n:]\n",
    "    ngram = ['<s>'] * (n - len(input_list))\n",
    "    ngram.extend(input_list)\n",
    "    return ngram\n",
    "# unit test\n",
    "print(generateNGram(['a','b'], 3))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO 请编写word_segment_viterbi函数来实现对输入字符串的分词\n",
    "def word_segment_viterbi(input_str, lm, n):\n",
    "    \"\"\"\n",
    "    1. 基于输入字符串，词典，以及给定的unigram概率来创建DAG(有向图）。\n",
    "    2. 编写维特比算法来寻找最优的PATH\n",
    "    3. 返回分词结果\n",
    "    \n",
    "    input_str: 输入字符串   输入格式：“今天天气好”\n",
    "    best_segment: 最好的分词结果  输出格式：[\"今天\"，\"天气\"，\"好\"]\n",
    "    \"\"\"\n",
    "    # boundary check  \n",
    "    if len(input_str) <= 0:\n",
    "        return [input_str]    \n",
    "    \n",
    "    # 1. generate the directed graph.\n",
    "    graph = generateGraph(input_str, lm.vocab)\n",
    "    \n",
    "    # TODO： 第二步： 利用维特比算法来找出最好的PATH， 这个PATH是P(sentence)最大或者 -log P(sentence)最小的PATH。\n",
    "    #              hint: 思考为什么不用相乘: p(w1)p(w2)...而是使用negative log sum:  -log(w1)-log(w2)-...\n",
    "    # init the dp\n",
    "    N = len(input_str)\n",
    "    path = [-1] * N # store the previous node.\n",
    "    dp = [0] * N    # store the dynamic program array.\n",
    "    cutlist = [[]] * N\n",
    "            \n",
    "    for i in range(N):\n",
    "        # iterate for each edge\n",
    "        dp[i] = 10e20\n",
    "        for e in graph[i]:\n",
    "            currentCut = cutlist[e][-(n - 1):] # deep copy\n",
    "            currentCut.append(input_str[e + 1:i + 1])\n",
    "            currentCut = generateNGram(currentCut, n)\n",
    "            score = dp[e] + lm.getScore(currentCut)\n",
    "            if score < dp[i]:\n",
    "                dp[i] = score\n",
    "                path[i] = e\n",
    "                cutlist[i] = currentCut\n",
    "\n",
    "    # TODO: 第三步： 根据最好的PATH, 返回最好的切分\n",
    "    idx = N - 1\n",
    "    best_segment = []\n",
    "    while idx != -1:\n",
    "        best_segment.append(input_str[path[idx] + 1:idx + 1])\n",
    "        idx = path[idx]\n",
    "\n",
    "    best_segment.reverse()\n",
    "    return best_segment   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['经', '馨']\n",
      "['经常', '有意', '见', '分歧', '?']\n",
      "['北京', '的', '天气', '，', '真', '好', '啊']\n",
      "['今天', '的', '课程', '内容', '很', '有意思']\n",
      "['']\n"
     ]
    }
   ],
   "source": [
    "print(word_segment_viterbi(\"经馨\", lm, 3))\n",
    "print(word_segment_viterbi(\"经常有意见分歧?\", lm, 3))\n",
    "print(word_segment_viterbi(\"北京的天气，真好啊\", lm, 3))\n",
    "print(word_segment_viterbi(\"今天的课程内容很有意思\", lm, 3))\n",
    "print(word_segment_viterbi(\"\", lm, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenized_viterbi(text, lm, n):\n",
    "    \"\"\" text tokenized interface.\n",
    "    Args:\n",
    "        text (str): the input text.\n",
    "        lm (languageModel): the input ngram language model\n",
    "        n (int): the level of the n-gram\n",
    "    Returns:\n",
    "        The tokenized list of the input text\n",
    "    \"\"\"\n",
    "    tokenized = [word_segment_viterbi(sent.strip(), lm, 3) for sent in text.split(' ')]\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['北京', '的', '天气', '，', '真', '好', '啊'], [''], [''], ['今天', '的', '课程', '内容', '很', '有意思']]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_viterbi(\"北京的天气，真好啊   今天的课程内容很有意思\", lm, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
