{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords # stopwords corpus\n",
    "from nltk.stem import PorterStemmer # stemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer # for BOW\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # for TF-IDF\n",
    "from gensim.models import Word2Vec # for Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "snow = nltk.stem.SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   .!sss.  \n"
     ]
    }
   ],
   "source": [
    "cleanr = re.compile('<.*?>') # remove html tag\n",
    "s = '<heml><body> .!sss.</body></html>'\n",
    "s = re.sub(cleanr, ' ', s)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    sss   \n"
     ]
    }
   ],
   "source": [
    "# remove puncuations\n",
    "s = re.sub(r'[?|!|\\'|\"|#]',r'',s)\n",
    "s = re.sub(r'[.|,|)|(|\\|/]',r' ',s)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sss  \n"
     ]
    }
   ],
   "source": [
    "# remove stopwords and stemming\n",
    "words = [snow.stem(word) for word in s.split() if word not in stopwords.words('english')] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "# test for join\n",
    "words = ['hello', 'world']\n",
    "sentence = (' ').join(words)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1\n",
      "  (0, 10)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 6)\t1\n",
      "  (2, 8)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 2)\t1\n",
      "  (2, 4)\t1\n",
      "  (2, 5)\t1\n",
      "  (2, 9)\t1\n",
      "  (2, 7)\t1\n"
     ]
    }
   ],
   "source": [
    "# encoding BOW\n",
    "corpus = ['hello world', 'i like pizza', 'the future is not our to see']\n",
    "count_vect = CountVectorizer(max_features=5000)\n",
    "bow_data = count_vect.fit_transform(corpus)\n",
    "print(bow_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t1\n",
      "  (0, 18)\t1\n",
      "  (0, 3)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 12)\t1\n",
      "  (1, 7)\t1\n",
      "  (2, 14)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 4)\t1\n",
      "  (2, 8)\t1\n",
      "  (2, 10)\t1\n",
      "  (2, 16)\t1\n",
      "  (2, 13)\t1\n",
      "  (2, 15)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 5)\t1\n",
      "  (2, 9)\t1\n",
      "  (2, 11)\t1\n",
      "  (2, 17)\t1\n"
     ]
    }
   ],
   "source": [
    "# train with Bi-Gram\n",
    "count_vect = CountVectorizer(ngram_range=(1,2))\n",
    "Bigram_data = count_vect.fit_transform(corpus)\n",
    "print(Bigram_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 10)\t0.7071067811865476\n",
      "  (0, 1)\t0.7071067811865476\n",
      "  (1, 6)\t0.7071067811865476\n",
      "  (1, 3)\t0.7071067811865476\n",
      "  (2, 7)\t0.37796447300922725\n",
      "  (2, 9)\t0.37796447300922725\n",
      "  (2, 5)\t0.37796447300922725\n",
      "  (2, 4)\t0.37796447300922725\n",
      "  (2, 2)\t0.37796447300922725\n",
      "  (2, 0)\t0.37796447300922725\n",
      "  (2, 8)\t0.37796447300922725\n"
     ]
    }
   ],
   "source": [
    "# train tf-idf\n",
    "tf_idf = TfidfVectorizer(max_features=5000)\n",
    "tf_data = tf_idf.fit_transform(corpus)\n",
    "print(tf_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec\n",
    "w2v_data = corpus\n",
    "splitted = []\n",
    "for row in w2v_data:\n",
    "    splitted.append([word for word in row.split()]) # splitting words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['hello', 'world'], ['i', 'like', 'pizza'], ['the', 'future', 'is', 'not', 'our', 'to', 'see']]\n"
     ]
    }
   ],
   "source": [
    "print(splitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_w2v = Word2Vec(splitted, min_count = 1, size = 10, workers =4 ) # default min_count = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "avg_data = []\n",
    "for row in splitted:\n",
    "    vec = np.zeros(10)\n",
    "    count = 0\n",
    "    for word in row:\n",
    "        try:\n",
    "            vec += train_w2v[word]\n",
    "            count += 1\n",
    "        except:\n",
    "            pass\n",
    "    avg_data.append(vec/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 0.0049132 ,  0.00257161,  0.00578847, -0.0263087 ,  0.00094173,\n",
      "        0.00911294, -0.03673651,  0.03076983,  0.01136076,  0.00464733]), array([-0.04138814, -0.00521938,  0.01968945, -0.00489506, -0.00946177,\n",
      "       -0.01072774, -0.03500661,  0.00763235, -0.02062522,  0.03836296]), array([ 0.00302578,  0.00668292, -0.00654146, -0.00297376,  0.01710409,\n",
      "        0.00249151, -0.01167992,  0.00896452,  0.0009047 ,  0.00048868])]\n"
     ]
    }
   ],
   "source": [
    "print(avg_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                    V = ( t(W1)*w2v(W1) + t(W2)*w2v(W2) +.....+t(Wn)*w2v(Wn))/(t(W1)+t(W2)+....+t(Wn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['future', 'hello', 'is', 'like', 'not', 'our', 'pizza', 'see', 'the', 'to', 'world']\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF word2vec\n",
    "tf_w_data = corpus\n",
    "tf_idf = TfidfVectorizer(max_features=5000)\n",
    "tf_idf_data = tf_idf.fit_transform(tf_w_data)\n",
    "print(tf_idf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 0.04207142, -0.00975782, -0.01198703,  0.03148258,  0.01852668,\n",
      "       -0.0347459 , -0.00663106,  0.0412101 , -0.04673776,  0.0484617 ]), array([ 0.04185987,  0.0347254 ,  0.04314079, -0.02122546,  0.01860863,\n",
      "        0.03253822, -0.02990762,  0.04165432, -0.01679715, -0.00473316]), array([-0.00444083,  0.03311056,  0.00554849, -0.02299906,  0.04853451,\n",
      "        0.02387154, -0.00649266, -0.03070207, -0.0243659 , -0.04736215])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "tf_w_data = []\n",
    "tf_idf_data  = tf_idf_data.toarray()\n",
    "vocab = tf_idf.get_feature_names()\n",
    "for i, row in zip(range(len(splitted)),splitted):\n",
    "    vec = [0] * 10\n",
    "    \n",
    "    tf_idf_sum = 0\n",
    "    for val in tf_idf_data[i]:\n",
    "        if val != 0:\n",
    "            tf_idf_sum += val\n",
    "            try:\n",
    "                vec += (val * train_w2v[vocab[i]])\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    vec = (float)(1 / tf_idf_sum) * vec\n",
    "    tf_w_data.append(vec)\n",
    "\n",
    "    \n",
    "print(tf_w_data)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
