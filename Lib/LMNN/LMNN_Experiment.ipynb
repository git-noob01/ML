{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LMNN accuracy on test set of 45 points: 0.9778\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from pylmnn import LargeMarginNearestNeighbor as LMNN\n",
    "\n",
    "\n",
    "# Load a data set\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# Split in training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "\n",
    "# Set up the hyperparameters\n",
    "k_train, k_test, n_components, max_iter = 3, 3, X.shape[1], 180\n",
    "\n",
    "# Instantiate the metric learner\n",
    "lmnn = LMNN(n_neighbors=k_train, max_iter=max_iter, n_components=n_components)\n",
    "\n",
    "# Train the metric learner\n",
    "lmnn.fit(X_train, y_train)\n",
    "\n",
    "# Fit the nearest neighbors classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=k_test)\n",
    "knn.fit(lmnn.transform(X_train), y_train)\n",
    "\n",
    "# Compute the k-nearest neighbor test f1-score after applying the learned transformation\n",
    "lmnn_acc = knn.score(lmnn.transform(X_test), y_test)\n",
    "print('LMNN accuracy on test set of {} points: {:.4f}'.format(X_test.shape[0], lmnn_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        15\n",
      "           1       0.94      1.00      0.97        15\n",
      "           2       1.00      0.93      0.97        15\n",
      "\n",
      "    accuracy                           0.98        45\n",
      "   macro avg       0.98      0.98      0.98        45\n",
      "weighted avg       0.98      0.98      0.98        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lmnn_pred = knn.predict(lmnn.transform(X_test))\n",
    "print(classification_report(y_test, lmnn_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on test set of 45 points: 0.9556\n"
     ]
    }
   ],
   "source": [
    "# test normal KNN\n",
    "clf = KNeighborsClassifier(n_neighbors=k_test)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "eucilidean_acc = clf.score(X_test, y_test)\n",
    "print('accuracy on test set of {} points: {:.4f}'.format(X_test.shape[0], eucilidean_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        15\n",
      "           1       0.88      1.00      0.94        15\n",
      "           2       1.00      0.87      0.93        15\n",
      "\n",
      "    accuracy                           0.96        45\n",
      "   macro avg       0.96      0.96      0.96        45\n",
      "weighted avg       0.96      0.96      0.96        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test CIFAR10 data set with grid search\n",
    "from load_data import load_CIFAR10\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cifar10_dir = 'cifar-10-batches-py'\n",
    "\n",
    "# clear variable \n",
    "try:\n",
    "   del X_train, y_train\n",
    "   del X_test, y_test\n",
    "   print('clean the variable which has been imported...Done!')\n",
    "except:\n",
    "   pass\n",
    "\n",
    "# read data and split into train and test\n",
    "X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data and test data: (50000, 32, 32, 3) (50000,) (10000, 32, 32, 3) (10000,)\n",
      "Labels:  [0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "print (\"Train Data and test data:\", X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "print (\"Labels: \", np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 32, 32, 3) (5000,) (1000, 32, 32, 3) (1000,)\n"
     ]
    }
   ],
   "source": [
    "# the dataset is too large, we need do a subsample.\n",
    "num_training = 5000\n",
    "num_test = 1000\n",
    "\n",
    "idx_train = np.random.randint(0, y_train.shape[0], num_training)\n",
    "idx_test = np.random.randint(0, y_test.shape[0], num_test)\n",
    "X_train = X_train[idx_train]\n",
    "y_train = y_train[idx_train]\n",
    "\n",
    "X_test = X_test[idx_test]\n",
    "y_test = y_test[idx_test]\n",
    "\n",
    "print (X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 3072) (1000, 3072)\n"
     ]
    }
   ],
   "source": [
    "# reshape the image data\n",
    "X_train1 = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "X_test1 = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "print(X_train1.shape, X_test1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  15 | elapsed:   45.9s remaining:   40.2s\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:  1.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': 1}\n",
      "Accuracy in the test data set is: 0.25\n"
     ]
    }
   ],
   "source": [
    "# test normal KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# 训练数据： （X_train1, y_train）, 测试数据：(X_test1, y_test)\n",
    "params_k = [1,3,5,7,9,12]  # 可以选择的K值\n",
    "# the p = 3 of the Minkowski Distance is very very slow for KNN,so we choose p = 2\n",
    "params = {'n_neighbors' : params_k}\n",
    "\n",
    "# 构建模型\n",
    "kf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 42)\n",
    "knn = KNeighborsClassifier() # ! Do not specify any parameters for the constructor\n",
    "model = GridSearchCV(knn, params,n_jobs= -1, cv = kf, verbose = 2) # 3-fold-cross-validation\n",
    "model.fit(X_train1, y_train)\n",
    "\n",
    "# print best parameters\n",
    "print(model.best_params_)\n",
    "\n",
    "# result\n",
    "print(\"Accuracy in the test data set is: %.2f\"%model.score(X_test1, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.24      0.24       109\n",
      "           1       0.52      0.12      0.20        98\n",
      "           2       0.16      0.32      0.21       104\n",
      "           3       0.18      0.13      0.15        97\n",
      "           4       0.20      0.35      0.26       113\n",
      "           5       0.26      0.19      0.22        99\n",
      "           6       0.23      0.22      0.22        96\n",
      "           7       0.27      0.15      0.19        87\n",
      "           8       0.35      0.58      0.44       102\n",
      "           9       0.50      0.11      0.17        95\n",
      "\n",
      "    accuracy                           0.25      1000\n",
      "   macro avg       0.29      0.24      0.23      1000\n",
      "weighted avg       0.29      0.25      0.23      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_pred = model.predict(X_test1)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the data before PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train1)\n",
    "X_train_normal = scaler.transform(X_train1)\n",
    "X_test_normal = scaler.transform(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we done PCA\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 32, svd_solver='full')\n",
    "X_train3 = pca.fit_transform(X_train_normal)\n",
    "# we do not need to re-fit the test data again in the same PCA model.\n",
    "X_test3 = pca.transform(X_test_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on test set of 1000 points: 0.3150\n"
     ]
    }
   ],
   "source": [
    "# retrain the data by KNN\n",
    "# test normal KNN\n",
    "clf = KNeighborsClassifier(n_neighbors=10, p = 2)\n",
    "clf.fit(X_train3, y_train)\n",
    "\n",
    "eucilidean_acc = clf.score(X_test3, y_test)\n",
    "print('accuracy on test set of {} points: {:.4f}'.format(X_test3.shape[0], eucilidean_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LMNN accuracy on test set of 1000 points: 0.3010\n"
     ]
    }
   ],
   "source": [
    "# train with mahalanobis\n",
    "# Instantiate the metric learner\n",
    "from pylmnn import LargeMarginNearestNeighbor as LMNN\n",
    "lmnn = LMNN(n_neighbors=10, max_iter=200, n_components=X_train1.shape[1])\n",
    "\n",
    "# Train the metric learner\n",
    "lmnn.fit(X_train1, y_train)\n",
    "\n",
    "# Fit the nearest neighbors classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=10, p = 2)\n",
    "knn.fit(lmnn.transform(X_train1), y_train)\n",
    "\n",
    "# Compute the k-nearest neighbor test f1-score after applying the learned transformation\n",
    "lmnn_acc = knn.score(lmnn.transform(X_test1), y_test)\n",
    "print('LMNN accuracy on test set of {} points: {:.4f}'.format(X_test1.shape[0], lmnn_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
